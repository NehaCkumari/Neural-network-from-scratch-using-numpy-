# -*- coding: utf-8 -*-
"""Neural network from scratch using numpy .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kAqnmaIdxsN5B91EqFEzu6tzHLDzU-sP
"""

import numpy as np

#Y->Actual_output
#Y is not seeded 
Y = (np.random.random((3,1))>0.5)*1

np.random.seed(0)

#X->inputs having 4 features and 3 training sets
X = np.random.random((3,4))


#number of hidden layers=2,number of nodes in hidden layers=6
training_set =3
learning_rate =0.01
training_epochs=100000
no_hidden_layers =2
nodes_hidden =6
no_in =4
no_out =1

#weights and biases
W1 = 2*np.random.random((no_in,nodes_hidden))-1
W2 = 2*np.random.random((nodes_hidden,nodes_hidden))-1
W3 = 2*np.random.random((nodes_hidden,no_out))-1
b1=np.ones((training_set,nodes_hidden))
b2=np.ones((training_set,nodes_hidden))
b3=np.ones((training_set,no_out))


#sigmoid function as an activation function
def sigmoid(z):
    return((1.0/(1.0+np.exp(-z))))
#sigmoid derivative
def sigmoid_derivative(a):
    return(np.multiply(a,(1-a)))

for epoch in range(training_epochs):
    #forward propagation----------------
    #layer1, layer2, layer3 and layer4 as a0,a1,a2,a3
    #z=weight*inputs+bias
    
    a0=X
    z1=np.add((np.dot(a0,W1)),b1)
    a1=sigmoid(z1)
    
    z2=np.add((np.dot(a1,W2)),b2)
    a2=sigmoid(z2)
    
    z3=np.add((np.dot(a2,W3)),b3)
    a3=sigmoid(z3)
              
              
    #backpropagation--------------------------
    
    #error in output layer as y_pred - Y
    #error in weights as dw(l+1)=(1/m)*(a(l).T*da(l+1))
    #w += -learning_rate * dw
    
    da3=np.subtract(a3,Y)
    dw3=np.divide((np.dot(a2.T,da3)),training_set)
    db3=np.divide((np.sum(da3,axis=1,keepdims=True)),training_set)
    W3+=-np.multiply(learning_rate,dw3)
    
    da2=np.multiply(np.dot(da3,W3.T),sigmoid_derivative(a2))
    dw2=np.divide((np.dot(a1.T,da2)),training_set)
    db2=np.divide((np.sum(da2,axis=1,keepdims=True)),training_set)
    W2+=-np.multiply(learning_rate,dw2)  
    
    da1=np.multiply(np.dot(da2,W2.T),sigmoid_derivative(a1))
    dw1=np.divide((np.dot(a0.T,da1)),training_set)
    db1=np.divide((np.sum(da1,axis=1,keepdims=True)),training_set)
    W1+=-np.multiply(learning_rate,dw1)
    
   

print("y_pred : ",(a3>0.5)*1,'\n')
print("Actual_output : ",Y)

